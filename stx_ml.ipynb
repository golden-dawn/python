{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STX ML\n",
    "To install:\n",
    "```\n",
    "mkvirtualenv venv-ml\n",
    "workon venv-ml\n",
    "pip install -r requirements.txt\n",
    "pip install -r requirements_ml.txt\n",
    "```\n",
    "Based on code from https://github.com/Kulbear/stock-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "stxcal.py:17: FutureWarning: pandas.core.datetools.Timedelta is deprecated. Please use pandas._libs.tslib.Timedelta instead.\n",
      "  pd.datetools.Timedelta(weeks=52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing calendar between 1901-01-01 and 2019-12-25\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as prep\n",
    "import stxcal \n",
    "from stxts import StxTS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Start with one stock: MSFT; adjust for splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8244"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk = 'MSFT'\n",
    "sd = '1960-01-01'\n",
    "ed = stxcal.current_date()\n",
    "ts = StxTS(stk, sd, ed)\n",
    "dt = str(ts.df.index[ts.l - 1].date())\n",
    "ts.set_day(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o</th>\n",
       "      <th>hi</th>\n",
       "      <th>lo</th>\n",
       "      <th>volume</th>\n",
       "      <th>open_interest</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-03-13</th>\n",
       "      <td>0.079994</td>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.079994</td>\n",
       "      <td>1.151131e+06</td>\n",
       "      <td>97190.94</td>\n",
       "      <td>0.084431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-14</th>\n",
       "      <td>0.084431</td>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.084431</td>\n",
       "      <td>3.446672e+05</td>\n",
       "      <td>31017.60</td>\n",
       "      <td>0.089993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-17</th>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.095555</td>\n",
       "      <td>0.089993</td>\n",
       "      <td>1.488118e+05</td>\n",
       "      <td>13392.00</td>\n",
       "      <td>0.089993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-18</th>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.095555</td>\n",
       "      <td>0.089993</td>\n",
       "      <td>7.424587e+04</td>\n",
       "      <td>6681.60</td>\n",
       "      <td>0.089993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-19</th>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.084431</td>\n",
       "      <td>5.376425e+04</td>\n",
       "      <td>4838.40</td>\n",
       "      <td>0.089993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   o        hi        lo        volume  open_interest  \\\n",
       "1986-03-13  0.079994  0.089993  0.079994  1.151131e+06       97190.94   \n",
       "1986-03-14  0.084431  0.089993  0.084431  3.446672e+05       31017.60   \n",
       "1986-03-17  0.089993  0.095555  0.089993  1.488118e+05       13392.00   \n",
       "1986-03-18  0.089993  0.095555  0.089993  7.424587e+04        6681.60   \n",
       "1986-03-19  0.089993  0.089993  0.084431  5.376425e+04        4838.40   \n",
       "\n",
       "                   c  \n",
       "1986-03-13  0.084431  \n",
       "1986-03-14  0.089993  \n",
       "1986-03-17  0.089993  \n",
       "1986-03-18  0.089993  \n",
       "1986-03-19  0.089993  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.df['open_interest'] = ts.df['c'] * ts.df['volume']\n",
    "ts.df = ts.df[['o', 'hi', 'lo', 'volume', 'open_interest', 'c']]\n",
    "ts.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data prior to feeding into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(X_train, X_test):\n",
    "    train_samples, train_nx, train_ny = X_train.shape\n",
    "    test_samples, test_nx, test_ny = X_test.shape\n",
    "    X_train = X_train.reshape((train_samples, train_nx * train_ny))\n",
    "    X_test = X_test.reshape((test_samples, test_nx * test_ny))\n",
    "    preprocessor = prep.StandardScaler().fit(X_train)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    \n",
    "    X_train = X_train.reshape((train_samples, train_nx, train_ny))\n",
    "    X_test = X_test.reshape((test_samples, test_nx, test_ny))\n",
    "    return X_train, X_test, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data intoX_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.values\n",
    "    \n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index : index + sequence_length])\n",
    "    result = np.array(result)\n",
    "    row = round(0.95 * result.shape[0])\n",
    "    train = result[: int(row), :]\n",
    "    \n",
    "    train, result, preprocessor = standard_scaler(train, result)\n",
    "    \n",
    "    X_train = train[:, : -1]\n",
    "    y_train = train[:, -1][: ,-1]\n",
    "    X_test = result[int(row) :, : -1]\n",
    "    y_test = result[int(row) :, -1][ : ,-1]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test, preprocessor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LSTM Network\n",
    "\n",
    "Here we will build a simple RNN with 2 LSTM layers.\n",
    "The architecture is:\n",
    "    \n",
    "    LSTM --> Dropout --> LSTM --> Dropout --> Fully-Connected(Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    # By setting return_sequences to True we are able to stack another LSTM layer\n",
    "    model.add(LSTM(\n",
    "        input_dim=layers[0],\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.35))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[3]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train', (7822, 10, 6))\n",
      "('y_train', (7822,))\n",
      "('X_test', (412, 10, 6))\n",
      "('y_test', (412,))\n"
     ]
    }
   ],
   "source": [
    "window = 10\n",
    "X_train, y_train, X_test, y_test, preprocessor = preprocess_data(ts.df, window)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cma/.envs/venv-ml/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "/home/cma/.envs/venv-ml/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=10, return_sequences=True, input_shape=(None, 6))`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Compilation Time : ', 0.017663955688476562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cma/.envs/venv-ml/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n"
     ]
    }
   ],
   "source": [
    "model = build_model([X_train.shape[2], window, 100, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7039 samples, validate on 783 samples\n",
      "Epoch 1/200\n",
      "7039/7039 [==============================] - 3s 359us/step - loss: 0.2265 - acc: 0.0000e+00 - val_loss: 1.3394 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0303 - acc: 0.0000e+00 - val_loss: 0.8660 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.6198 - val_acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.6359 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0222 - acc: 0.0000e+00 - val_loss: 0.4054 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.5101 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0209 - acc: 0.0000e+00 - val_loss: 0.2634 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.4485 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.2294 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "7039/7039 [==============================] - 1s 134us/step - loss: 0.0204 - acc: 0.0000e+00 - val_loss: 0.3429 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "7039/7039 [==============================] - 1s 136us/step - loss: 0.0198 - acc: 0.0000e+00 - val_loss: 0.2152 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "7039/7039 [==============================] - 1s 134us/step - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.3810 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "7039/7039 [==============================] - 1s 132us/step - loss: 0.0193 - acc: 0.0000e+00 - val_loss: 0.1666 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "7039/7039 [==============================] - 1s 156us/step - loss: 0.0197 - acc: 0.0000e+00 - val_loss: 0.3160 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "7039/7039 [==============================] - 1s 140us/step - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "7039/7039 [==============================] - 1s 144us/step - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.2107 - val_acc: 0.0000e+00\n",
      "Epoch 17/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1164 - val_acc: 0.0000e+00\n",
      "Epoch 18/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1692 - val_acc: 0.0000e+00\n",
      "Epoch 19/200\n",
      "7039/7039 [==============================] - 1s 151us/step - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.0828 - val_acc: 0.0000e+00\n",
      "Epoch 20/200\n",
      "7039/7039 [==============================] - 1s 150us/step - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1964 - val_acc: 0.0000e+00\n",
      "Epoch 21/200\n",
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.0757 - val_acc: 0.0000e+00\n",
      "Epoch 22/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.2184 - val_acc: 0.0000e+00\n",
      "Epoch 23/200\n",
      "7039/7039 [==============================] - 1s 133us/step - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1539 - val_acc: 0.0000e+00\n",
      "Epoch 24/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.2850 - val_acc: 0.0000e+00\n",
      "Epoch 25/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1078 - val_acc: 0.0000e+00\n",
      "Epoch 26/200\n",
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1679 - val_acc: 0.0000e+00\n",
      "Epoch 27/200\n",
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.0771 - val_acc: 0.0000e+00\n",
      "Epoch 28/200\n",
      "7039/7039 [==============================] - 1s 137us/step - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1789 - val_acc: 0.0000e+00\n",
      "Epoch 29/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.0895 - val_acc: 0.0000e+00\n",
      "Epoch 30/200\n",
      "7039/7039 [==============================] - 1s 133us/step - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1109 - val_acc: 0.0000e+00\n",
      "Epoch 31/200\n",
      "7039/7039 [==============================] - 1s 132us/step - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.0895 - val_acc: 0.0000e+00\n",
      "Epoch 32/200\n",
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1376 - val_acc: 0.0000e+00\n",
      "Epoch 33/200\n",
      "7039/7039 [==============================] - 1s 137us/step - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 34/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.2271 - val_acc: 0.0000e+00\n",
      "Epoch 35/200\n",
      "7039/7039 [==============================] - 1s 148us/step - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.0915 - val_acc: 0.0000e+00\n",
      "Epoch 36/200\n",
      "7039/7039 [==============================] - 1s 138us/step - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1366 - val_acc: 0.0000e+00\n",
      "Epoch 37/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.0822 - val_acc: 0.0000e+00\n",
      "Epoch 38/200\n",
      "7039/7039 [==============================] - 1s 134us/step - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1967 - val_acc: 0.0000e+00\n",
      "Epoch 39/200\n",
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.0902 - val_acc: 0.0000e+00\n",
      "Epoch 40/200\n",
      "7039/7039 [==============================] - 1s 133us/step - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1925 - val_acc: 0.0000e+00\n",
      "Epoch 41/200\n",
      "7039/7039 [==============================] - 1s 136us/step - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.0711 - val_acc: 0.0000e+00\n",
      "Epoch 42/200\n",
      "7039/7039 [==============================] - 1s 134us/step - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1778 - val_acc: 0.0000e+00\n",
      "Epoch 43/200\n",
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.0744 - val_acc: 0.0000e+00\n",
      "Epoch 44/200\n",
      "7039/7039 [==============================] - 1s 136us/step - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0000e+00\n",
      "Epoch 45/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.0796 - val_acc: 0.0000e+00\n",
      "Epoch 46/200\n",
      "7039/7039 [==============================] - 1s 134us/step - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.2212 - val_acc: 0.0000e+00\n",
      "Epoch 47/200\n",
      "7039/7039 [==============================] - 1s 132us/step - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.0670 - val_acc: 0.0000e+00\n",
      "Epoch 48/200\n",
      "7039/7039 [==============================] - 1s 139us/step - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.0958 - val_acc: 0.0000e+00\n",
      "Epoch 49/200\n",
      "7039/7039 [==============================] - 1s 136us/step - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
      "Epoch 50/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1584 - val_acc: 0.0000e+00\n",
      "Epoch 51/200\n",
      "7039/7039 [==============================] - 1s 146us/step - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.0464 - val_acc: 0.0000e+00\n",
      "Epoch 52/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.0922 - val_acc: 0.0000e+00\n",
      "Epoch 53/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.0602 - val_acc: 0.0000e+00\n",
      "Epoch 54/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1429 - val_acc: 0.0000e+00\n",
      "Epoch 55/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.0643 - val_acc: 0.0000e+00\n",
      "Epoch 56/200\n",
      "7039/7039 [==============================] - 1s 133us/step - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1682 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.0687 - val_acc: 0.0000e+00\n",
      "Epoch 58/200\n",
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1323 - val_acc: 0.0000e+00\n",
      "Epoch 59/200\n",
      "7039/7039 [==============================] - 1s 145us/step - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.0539 - val_acc: 0.0000e+00\n",
      "Epoch 60/200\n",
      "7039/7039 [==============================] - 1s 207us/step - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1754 - val_acc: 0.0000e+00\n",
      "Epoch 61/200\n",
      "7039/7039 [==============================] - 1s 163us/step - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.0515 - val_acc: 0.0000e+00\n",
      "Epoch 62/200\n",
      "7039/7039 [==============================] - 1s 180us/step - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1924 - val_acc: 0.0000e+00\n",
      "Epoch 63/200\n",
      "7039/7039 [==============================] - 1s 163us/step - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.0765 - val_acc: 0.0000e+00\n",
      "Epoch 64/200\n",
      "7039/7039 [==============================] - 1s 138us/step - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.0974 - val_acc: 0.0000e+00\n",
      "Epoch 65/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.0286 - val_acc: 0.0000e+00\n",
      "Epoch 66/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.0705 - val_acc: 0.0000e+00\n",
      "Epoch 67/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.0385 - val_acc: 0.0000e+00\n",
      "Epoch 68/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1568 - val_acc: 0.0000e+00\n",
      "Epoch 69/200\n",
      "7039/7039 [==============================] - 1s 133us/step - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.0511 - val_acc: 0.0000e+00\n",
      "Epoch 70/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
      "Epoch 71/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.0316 - val_acc: 0.0000e+00\n",
      "Epoch 72/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.0798 - val_acc: 0.0000e+00\n",
      "Epoch 73/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.0455 - val_acc: 0.0000e+00\n",
      "Epoch 74/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1414 - val_acc: 0.0000e+00\n",
      "Epoch 75/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.0531 - val_acc: 0.0000e+00\n",
      "Epoch 76/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1420 - val_acc: 0.0000e+00\n",
      "Epoch 77/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.0677 - val_acc: 0.0000e+00\n",
      "Epoch 78/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1329 - val_acc: 0.0000e+00\n",
      "Epoch 79/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0000e+00\n",
      "Epoch 80/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.0629 - val_acc: 0.0000e+00\n",
      "Epoch 81/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0292 - val_acc: 0.0000e+00\n",
      "Epoch 82/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.0776 - val_acc: 0.0000e+00\n",
      "Epoch 83/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.0479 - val_acc: 0.0000e+00\n",
      "Epoch 84/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.0709 - val_acc: 0.0000e+00\n",
      "Epoch 85/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
      "Epoch 86/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.0937 - val_acc: 0.0000e+00\n",
      "Epoch 87/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.0550 - val_acc: 0.0000e+00\n",
      "Epoch 88/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1133 - val_acc: 0.0000e+00\n",
      "Epoch 89/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.0349 - val_acc: 0.0000e+00\n",
      "Epoch 90/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0797 - val_acc: 0.0000e+00\n",
      "Epoch 91/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.0308 - val_acc: 0.0000e+00\n",
      "Epoch 92/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1049 - val_acc: 0.0000e+00\n",
      "Epoch 93/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.0349 - val_acc: 0.0000e+00\n",
      "Epoch 94/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.0948 - val_acc: 0.0000e+00\n",
      "Epoch 95/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
      "Epoch 96/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0729 - val_acc: 0.0000e+00\n",
      "Epoch 97/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0478 - val_acc: 0.0000e+00\n",
      "Epoch 98/200\n",
      "7039/7039 [==============================] - 1s 135us/step - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0979 - val_acc: 0.0000e+00\n",
      "Epoch 99/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.0432 - val_acc: 0.0000e+00\n",
      "Epoch 100/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.0898 - val_acc: 0.0000e+00\n",
      "Epoch 101/200\n",
      "7039/7039 [==============================] - 1s 134us/step - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
      "Epoch 102/200\n",
      "7039/7039 [==============================] - 1s 139us/step - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.0474 - val_acc: 0.0000e+00\n",
      "Epoch 103/200\n",
      "7039/7039 [==============================] - 1s 133us/step - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0252 - val_acc: 0.0000e+00\n",
      "Epoch 104/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.0308 - val_acc: 0.0000e+00\n",
      "Epoch 105/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
      "Epoch 106/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
      "Epoch 107/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 108/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.0694 - val_acc: 0.0000e+00\n",
      "Epoch 109/200\n",
      "7039/7039 [==============================] - 1s 132us/step - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.0266 - val_acc: 0.0000e+00\n",
      "Epoch 110/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.0810 - val_acc: 0.0000e+00\n",
      "Epoch 111/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
      "Epoch 112/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.0590 - val_acc: 0.0000e+00\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7039/7039 [==============================] - 1s 131us/step - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0282 - val_acc: 0.0000e+00\n",
      "Epoch 114/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0582 - val_acc: 0.0000e+00\n",
      "Epoch 115/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 116/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0991 - val_acc: 0.0000e+00\n",
      "Epoch 117/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0313 - val_acc: 0.0000e+00\n",
      "Epoch 118/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0686 - val_acc: 0.0000e+00\n",
      "Epoch 119/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.0297 - val_acc: 0.0000e+00\n",
      "Epoch 120/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.0460 - val_acc: 0.0000e+00\n",
      "Epoch 121/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.0303 - val_acc: 0.0000e+00\n",
      "Epoch 122/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.0983 - val_acc: 0.0000e+00\n",
      "Epoch 123/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
      "Epoch 124/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.0640 - val_acc: 0.0000e+00\n",
      "Epoch 125/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0000e+00\n",
      "Epoch 126/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.0581 - val_acc: 0.0000e+00\n",
      "Epoch 127/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0467 - val_acc: 0.0000e+00\n",
      "Epoch 128/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0819 - val_acc: 0.0000e+00\n",
      "Epoch 129/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0350 - val_acc: 0.0000e+00\n",
      "Epoch 130/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.0792 - val_acc: 0.0000e+00\n",
      "Epoch 131/200\n",
      "7039/7039 [==============================] - 1s 122us/step - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.0326 - val_acc: 0.0000e+00\n",
      "Epoch 132/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.0660 - val_acc: 0.0000e+00\n",
      "Epoch 133/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.0318 - val_acc: 0.0000e+00\n",
      "Epoch 134/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.0743 - val_acc: 0.0000e+00\n",
      "Epoch 135/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0431 - val_acc: 0.0000e+00\n",
      "Epoch 136/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0994 - val_acc: 0.0000e+00\n",
      "Epoch 137/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0420 - val_acc: 0.0000e+00\n",
      "Epoch 138/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0788 - val_acc: 0.0000e+00\n",
      "Epoch 139/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.0323 - val_acc: 0.0000e+00\n",
      "Epoch 140/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.0639 - val_acc: 0.0000e+00\n",
      "Epoch 141/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0338 - val_acc: 0.0000e+00\n",
      "Epoch 142/200\n",
      "7039/7039 [==============================] - 1s 124us/step - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.0504 - val_acc: 0.0000e+00\n",
      "Epoch 143/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.0325 - val_acc: 0.0000e+00\n",
      "Epoch 144/200\n",
      "7039/7039 [==============================] - 1s 130us/step - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0683 - val_acc: 0.0000e+00\n",
      "Epoch 145/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0493 - val_acc: 0.0000e+00\n",
      "Epoch 146/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0954 - val_acc: 0.0000e+00\n",
      "Epoch 147/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0497 - val_acc: 0.0000e+00\n",
      "Epoch 148/200\n",
      "7039/7039 [==============================] - 1s 125us/step - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0905 - val_acc: 0.0000e+00\n",
      "Epoch 149/200\n",
      "7039/7039 [==============================] - 1s 123us/step - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0382 - val_acc: 0.0000e+00\n",
      "Epoch 150/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 151/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0568 - val_acc: 0.0000e+00\n",
      "Epoch 152/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0000e+00\n",
      "Epoch 153/200\n",
      "7039/7039 [==============================] - 1s 128us/step - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0661 - val_acc: 0.0000e+00\n",
      "Epoch 154/200\n",
      "7039/7039 [==============================] - 1s 127us/step - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0360 - val_acc: 0.0000e+00\n",
      "Epoch 155/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.1000 - val_acc: 0.0000e+00\n",
      "Epoch 156/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0342 - val_acc: 0.0000e+00\n",
      "Epoch 157/200\n",
      "7039/7039 [==============================] - 1s 129us/step - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.0891 - val_acc: 0.0000e+00\n",
      "Epoch 158/200\n",
      "7039/7039 [==============================] - 1s 126us/step - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0400 - val_acc: 0.0000e+00\n",
      "Epoch 159/200\n",
      "5120/7039 [====================>.........] - ETA: 0s - loss: 0.0091 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=1024,\n",
    "    epochs=200,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813/7813 [==============================] - 2s 291us/step\n",
      "Train Score: 0.43 MSE (0.65 RMSE)\n",
      "411/411 [==============================] - 0s 345us/step\n",
      "Test Score: 16.97 MSE (4.12 RMSE)\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "testScore = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "ratio = []\n",
    "pred = model.predict(X_test)\n",
    "for u in range(len(y_test)):\n",
    "    pr = pred[u][0]\n",
    "    ratio.append((y_test[u] / pr) - 1)\n",
    "    diff.append(abs(y_test[u] - pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if using within a jupyter notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot(pred[180:250], color='red', label='Prediction')\n",
    "plt2.plot(y_test[180:250], color='blue', label='Ground Truth')\n",
    "plt2.legend(loc='upper left')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
